{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6fc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "Max_shape_0=256\n",
    "Max_shape_1=256\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e6407",
   "metadata": {},
   "source": [
    "# load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771714d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "PATH = \"./qsub220208/Model_Resnet18_nomask.pkl\"\n",
    "model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1792d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "imgdir0120 = \"../../Datasets/211202NDAcquisition/CellsNoMask/NDAcquisition-01/\"\n",
    "imgdir0220 = \"../../Datasets/211202NDAcquisition/CellsNoMask/NDAcquisition-02Nami_x20/\"\n",
    "imgdir0140 = \"../../Datasets/211202NDAcquisition/CellsNoMask/NDAcquisition-01x40/\"\n",
    "imgdir0240 = \"../../Datasets/211202NDAcquisition/CellsNoMask/NDAcquisition-02Nami_x40/\"\n",
    "\n",
    "imgnamelist = [\"NDAcquisition-01_XY001_1\",\"NDAcquisition-01_XY001_8\",\n",
    "           \"NDAcquisition-01x40_XY0001_1\",\"NDAcquisition-01x40_XY0001_11\",\n",
    "           \"NDAcquisition-02Nami_x20_XY009_1\",\"NDAcquisition-02Nami_x20_XY136_2\",\n",
    "           \"NDAcquisition-02Nami_x40_XY001_2\",\"NDAcquisition-02Nami_x40_XY127_2\"]\n",
    "\n",
    "imgdirlist = [imgdir0120,imgdir0120,imgdir0140,imgdir0140,imgdir0220,imgdir0220,imgdir0240,imgdir0240]\n",
    "label=[0,0,0,0,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720c50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义钩子函数，获取指定层名称的特征\n",
    "feature_activation = {} # 保存获取的输出\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        feature_activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "def pad(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    imgSize = img.shape\n",
    "    top_size,bottom_size = (Max_shape_0-imgSize[0])//2,(Max_shape_0-imgSize[0])//2\n",
    "    left_size,right_size = (Max_shape_1-imgSize[1])//2,(Max_shape_1-imgSize[1])//2\n",
    "    if (imgSize[0] % 2) != 0:\n",
    "        top_size,bottom_size = (Max_shape_0-imgSize[0])//2,(Max_shape_0-imgSize[0])//2+1\n",
    "    if (imgSize[1] % 2) != 0:     \n",
    "        left_size,right_size = (Max_shape_1-imgSize[1])//2,(Max_shape_1-imgSize[1])//2+1\n",
    "    imgpad = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
    "    return imgpad\n",
    "\n",
    "\n",
    "def dataTransform(img_path):\n",
    "    imgpad = pad(img_path)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_img = transform(imgpad).unsqueeze(0).to(device)\n",
    "    return input_img\n",
    "\n",
    "\n",
    "def getfeature(img_path,plt):\n",
    "    img = cv2.imread(img_path)\n",
    "    imgSize = img.shape\n",
    "    top_size,bottom_size = (Max_shape_0-imgSize[0])//2,(Max_shape_0-imgSize[0])//2\n",
    "    left_size,right_size = (Max_shape_1-imgSize[1])//2,(Max_shape_1-imgSize[1])//2\n",
    "    if (imgSize[0] % 2) != 0:\n",
    "        top_size,bottom_size = (Max_shape_0-imgSize[0])//2,(Max_shape_0-imgSize[0])//2+1\n",
    "    if (imgSize[1] % 2) != 0:     \n",
    "        left_size,right_size = (Max_shape_1-imgSize[1])//2,(Max_shape_1-imgSize[1])//2+1\n",
    "    imgpad = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_img = transform(imgpad).unsqueeze(0).to(device)\n",
    "    for name, layer in model.named_modules():\n",
    "        layer.register_forward_hook(get_activation(name))\n",
    "    output = model(input_img)\n",
    "    feature_activation.pop(\"module.avgpool\")\n",
    "    feature_activation.pop(\"module.fc.0\")\n",
    "    feature_activation.pop(\"module.fc.1\")\n",
    "    feature_activation.pop(\"module.fc.2\")\n",
    "    feature_activation.pop(\"module.fc.3\")\n",
    "    feature_activation.pop(\"module.fc.4\")\n",
    "    feature_activation.pop(\"module.fc\")\n",
    "    feature_activation.pop(\"module\")\n",
    "    feature_activation.pop(\"\")\n",
    "    \n",
    "    if plt == True:\n",
    "        for key in feature_activation:\n",
    "            bn = feature_activation[key].cpu()\n",
    "            print(key,\" : \",bn.shape)\n",
    "            s = int(imgpad.shape[0]/bn.shape[2])\n",
    "            n = math.ceil(math.sqrt(bn.shape[1]))\n",
    "            plt.figure(figsize=(20,20))\n",
    "            for i in range(bn.shape[1]):\n",
    "                plt.subplot(n,n,i+1)\n",
    "                plt.imshow(bn[0,i,\n",
    "                                  int(top_size/s):int((top_size+imgSize[0])/s),\n",
    "                                  int(left_size/s):int((left_size+imgSize[1])/s)], cmap='gray')\n",
    "                plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbba9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True])   Real:  0 ;   Predict:  tensor([0])\n",
      "tensor([True])   Real:  0 ;   Predict:  tensor([0])\n",
      "tensor([True])   Real:  0 ;   Predict:  tensor([0])\n",
      "tensor([False])   Real:  0 ;   Predict:  tensor([1])\n",
      "tensor([True])   Real:  1 ;   Predict:  tensor([1])\n",
      "tensor([True])   Real:  1 ;   Predict:  tensor([1])\n",
      "tensor([True])   Real:  1 ;   Predict:  tensor([1])\n",
      "tensor([True])   Real:  1 ;   Predict:  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# check predict label correct or not\n",
    "for i in range(8):\n",
    "    imgdir = imgdirlist[i]\n",
    "    imgname = imgnamelist[i]\n",
    "    img_path = imgdir + imgname + \".tif\"\n",
    "    imgpad = pad(img_path)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_img = transform(imgpad).unsqueeze(0).to(device)\n",
    "    \n",
    "    output = model(input_img)\n",
    "    y=output.argmax(1).cpu()\n",
    "    print(y==label[i],\"  Real: \",label[i],\";   Predict: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965f3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "imgdir = imgdirlist[i]\n",
    "imgname = imgnamelist[i]\n",
    "img_path = imgdir + imgname + \".tif\"\n",
    "getfeature(img_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e95f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.conv1  :  torch.Size([1, 64, 128, 128])\n",
      "module.bn1  :  torch.Size([1, 64, 128, 128])\n",
      "module.relu  :  torch.Size([1, 64, 128, 128])\n",
      "module.maxpool  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.0.conv1  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.0.bn1  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.0.relu  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.0.conv2  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.0.bn2  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.0  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.1.conv1  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.1.bn1  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.1.relu  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.1.conv2  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.1.bn2  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1.1  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer1  :  torch.Size([1, 64, 64, 64])\n",
      "module.layer2.0.conv1  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.bn1  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.relu  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.conv2  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.bn2  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.downsample.0  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.downsample.1  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0.downsample  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.0  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.1.conv1  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.1.bn1  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.1.relu  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.1.conv2  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.1.bn2  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2.1  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer2  :  torch.Size([1, 128, 32, 32])\n",
      "module.layer3.0.conv1  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.bn1  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.relu  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.conv2  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.bn2  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.downsample.0  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.downsample.1  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0.downsample  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.0  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.1.conv1  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.1.bn1  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.1.relu  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.1.conv2  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.1.bn2  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3.1  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer3  :  torch.Size([1, 256, 16, 16])\n",
      "module.layer4.0.conv1  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.bn1  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.relu  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.conv2  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.bn2  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.downsample.0  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.downsample.1  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0.downsample  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.0  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.1.conv1  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.1.bn1  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.1.relu  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.1.conv2  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.1.bn2  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4.1  :  torch.Size([1, 512, 8, 8])\n",
      "module.layer4  :  torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "for key in feature_activation:\n",
    "    bn = feature_activation[key].cpu()\n",
    "    print(key,\" : \",bn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,8):\n",
    "    imgdir = imgdirlist[i]\n",
    "    imgname = imgnamelist[i]\n",
    "    img_path = imgdir + imgname + \".tif\"\n",
    "    getfeature(img_path,False)\n",
    "    imgpad,input_img,imgSize,top_size,bottom_size,left_size,right_size = dataTransform(img_path)\n",
    "    \n",
    "     \n",
    "    for key in feature_activation:\n",
    "        bn = feature_activation[key].cpu()\n",
    "        bn = np.array(bn)\n",
    "        s = int(imgpad.shape[0]/bn.shape[2])\n",
    "        n = math.ceil(math.sqrt(bn.shape[1]))\n",
    "        plt.figure(figsize=(20,20))\n",
    "        \n",
    "        # save path\n",
    "        path = \"./Feature18/\" + imgname + \"/\" + str(key)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        for i in range(bn.shape[1]):\n",
    "            # save each feature in each layers\n",
    "            savename = path+\"/\"+str(key)+\"_\"+str(i)+\".png\"\n",
    "            plt.imsave(savename, bn[0,i,int(top_size/s):int((top_size+imgSize[0])/s),\n",
    "                                  int(left_size/s):int((left_size+imgSize[1])/s)])\n",
    "        \n",
    "            # save all feature in each layers\n",
    "            plt.subplot(n,n,i+1)\n",
    "            plt.imshow(bn[0,i,int(top_size/s):int((top_size+imgSize[0])/s),\n",
    "                              int(left_size/s):int((left_size+imgSize[1])/s)], cmap='gray')\n",
    "        savename = path+\"/\"+str(key)+\".png\"\n",
    "        plt.savefig(savename)\n",
    "        print(\"Successfully saved \", savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbb9e9",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm ./qsub220209/FeatureView/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f5b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "img01='../../Datasets/211202NDAcquisition/CellsNoMask/NDAcquisition-01x40/NDAcquisition-01x40_XY0001_11.tif'\n",
    "getfeature(img01,False)"
   ],
   "id": "e961cbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009701fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in feature_activation:\n",
    "    print(\">>> \",key)\n",
    "    n = 1\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i in range(0,8,2):\n",
    "        img=cv2.imread(\"./qsub220209/Feature18/\"+imgnamelist[i]+\"/\"+key+\".png\")\n",
    "        plt.subplot(2,2,n)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        n+=1\n",
    "#     plt.show()\n",
    "    plt.savefig(\"./qsub220209/FeatureView/\"+key+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdd44d",
   "metadata": {},
   "source": [
    "# HeatMap可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec83594",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321db608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb4ff5d",
   "metadata": {},
   "source": [
    "## Save all feature map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3bc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8):\n",
    "#     imgdir = imgdirlist[i]\n",
    "#     imgname = imgnamelist[i]\n",
    "#     img_path = imgdir + imgname + \".tif\"\n",
    "#     getfeature(img_path,False)\n",
    "#     imgpad,input_img,imgSize,top_size,bottom_size,left_size,right_size = dataTransform(img_path)\n",
    "    \n",
    "     \n",
    "#     for key in feature_activation:\n",
    "#         bn = feature_activation[key].cpu()\n",
    "#         bn = np.array(bn)\n",
    "#         s = int(imgpad.shape[0]/bn.shape[2])\n",
    "#         n = math.ceil(math.sqrt(bn.shape[1]))\n",
    "#         plt.figure(figsize=(20,20))\n",
    "        \n",
    "#         # save path\n",
    "#         path = \"./Feature18/\" + imgname + \"/\" + str(key)\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "            \n",
    "#         for i in range(bn.shape[1]):\n",
    "#             # save each feature in each layers\n",
    "#             savename = path+\"/\"+str(key)+\"_\"+str(i)+\".png\"\n",
    "#             plt.imsave(savename, bn[0,i,int(top_size/s):int((top_size+imgSize[0])/s),\n",
    "#                                   int(left_size/s):int((left_size+imgSize[1])/s)])\n",
    "        \n",
    "#             # save all feature in each layers\n",
    "#             plt.subplot(n,n,i+1)\n",
    "#             plt.imshow(bn[0,i,int(top_size/s):int((top_size+imgSize[0])/s),\n",
    "#                               int(left_size/s):int((left_size+imgSize[1])/s)], cmap='gray')\n",
    "#         savename = path+\"/\"+str(key)+\".png\"\n",
    "#         plt.savefig(savename)\n",
    "#         print(\"Successfully saved \", savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb81df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
